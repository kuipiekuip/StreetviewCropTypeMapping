{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b7c7aa67",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2024-04-02T10:16:00.504684Z",
     "iopub.status.busy": "2024-04-02T10:16:00.503899Z",
     "iopub.status.idle": "2024-04-02T10:16:00.837732Z",
     "shell.execute_reply": "2024-04-02T10:16:00.836136Z"
    },
    "papermill": {
     "duration": 0.341498,
     "end_time": "2024-04-02T10:16:00.839577",
     "exception": true,
     "start_time": "2024-04-02T10:16:00.498079",
     "status": "failed"
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'test'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 74\u001b[0m\n\u001b[1;32m     51\u001b[0m base_dir \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m  \u001b[38;5;66;03m# Update this to your base directory path\u001b[39;00m\n\u001b[1;32m     52\u001b[0m \u001b[38;5;66;03m#directory should look as here below\u001b[39;00m\n\u001b[1;32m     53\u001b[0m \n\u001b[1;32m     54\u001b[0m \u001b[38;5;66;03m# directory\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     68\u001b[0m \u001b[38;5;66;03m# -- image 2\u001b[39;00m\n\u001b[1;32m     69\u001b[0m \u001b[38;5;66;03m# -- image 3 ...\u001b[39;00m\n\u001b[0;32m---> 74\u001b[0m \u001b[43mprocess_directories\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbase_dir\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[1], line 35\u001b[0m, in \u001b[0;36mprocess_directories\u001b[0;34m(base_dir)\u001b[0m\n\u001b[1;32m     32\u001b[0m current_dir \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(base_dir, sub_dir)\n\u001b[1;32m     34\u001b[0m \u001b[38;5;66;03m# Process each JSON file in the directory\u001b[39;00m\n\u001b[0;32m---> 35\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m \u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlistdir\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcurrent_dir\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m     36\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m file\u001b[38;5;241m.\u001b[39mendswith(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.json\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[1;32m     37\u001b[0m         json_file_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(current_dir, file)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'test'"
     ]
    }
   ],
   "source": [
    "\n",
    "#####\n",
    "#####\n",
    "#####  All training data you can find in TreeOrNoTree-2.zip\n",
    "\n",
    "import os\n",
    "import json\n",
    "import csv\n",
    "\n",
    "# Function to extract filenames and labels from JSON annotations\n",
    "def extract_filenames_and_labels(json_file):\n",
    "    with open(json_file) as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    filenames = []\n",
    "    labels = []\n",
    "\n",
    "    # Map of image IDs to filenames\n",
    "    image_id_to_filename = {image['id']: image['file_name'] for image in data['images']}\n",
    "\n",
    "    # Set of image IDs that have trees\n",
    "    image_ids_with_trees = set(annotation['image_id'] for annotation in data['annotations'] if annotation['category_id'] == 1)\n",
    "\n",
    "    # Generate filenames and labels\n",
    "    for image_id, filename in image_id_to_filename.items():\n",
    "        filenames.append(filename)\n",
    "        labels.append(1 if image_id in image_ids_with_trees else 0)\n",
    "\n",
    "    return filenames, labels\n",
    "\n",
    "# Main function to process directories and generate CSV files\n",
    "def process_directories(base_dir):\n",
    "    # Sub-directories to process\n",
    "    sub_dirs = ['test', 'train', 'valid']\n",
    "\n",
    "    for sub_dir in sub_dirs:\n",
    "        current_dir = os.path.join(base_dir, sub_dir)\n",
    "        \n",
    "        # Process each JSON file in the directory\n",
    "        for file in os.listdir(current_dir):\n",
    "            if file.endswith('.json'):\n",
    "                json_file_path = os.path.join(current_dir, file)\n",
    "                filenames, labels = extract_filenames_and_labels(json_file_path)\n",
    "\n",
    "                # Create a corresponding CSV file\n",
    "                csv_file_path = os.path.join(current_dir, os.path.splitext(file)[0] + '.csv')\n",
    "                with open(csv_file_path, mode='w', newline='') as csv_file:\n",
    "                    writer = csv.writer(csv_file)\n",
    "                    writer.writerow(['filename', 'label'])  # Write header\n",
    "                    for filename, label in zip(filenames, labels):\n",
    "                        writer.writerow([filename, label])\n",
    "\n",
    "                print(f\"Processed {json_file_path} -> {csv_file_path}\")\n",
    "\n",
    "# Example usage\n",
    "base_dir = ''  # Update this to your base directory path\n",
    "#directory should look as here below\n",
    "\n",
    "# directory\n",
    "# -test\n",
    "# -- annotation.coco.json \n",
    "# -- image 1\n",
    "# -- image 2\n",
    "# -- image 3 ...\n",
    "# -train\n",
    "# -- annotation.coco.json \n",
    "# -- image 1\n",
    "# -- image 2\n",
    "# -- image 3 ...\n",
    "# -validate\n",
    "# -- annotation.coco.json \n",
    "# -- image 1\n",
    "# -- image 2\n",
    "# -- image 3 ...\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "process_directories(base_dir)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "393efa14",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-02T09:10:19.935720Z",
     "iopub.status.busy": "2024-04-02T09:10:19.935421Z",
     "iopub.status.idle": "2024-04-02T09:10:19.954849Z",
     "shell.execute_reply": "2024-04-02T09:10:19.954105Z",
     "shell.execute_reply.started": "2024-04-02T09:10:19.935695Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import csv\n",
    "import torch\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from PIL import Image\n",
    "\n",
    "# Define paths\n",
    "base_dir = \"\" # add directory as descirbed in cell here above\n",
    "sub_dirs = {\n",
    "    \"train\": \"train\",\n",
    "    \"valid\": \"valid\",\n",
    "    \"test\": \"test\"\n",
    "}\n",
    "\n",
    "# Custom Dataset Class\n",
    "class TreeDataset(Dataset):\n",
    "    def __init__(self, csv_file, img_dir, transform=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            csv_file (string): Path to the csv file with annotations.\n",
    "            img_dir (string): Directory with all the images.\n",
    "            transform (callable, optional): Optional transform to be applied\n",
    "                on a sample.\n",
    "        \"\"\"\n",
    "        self.tree_frame = pd.read_csv(csv_file)\n",
    "        self.img_dir = img_dir\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.tree_frame)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "\n",
    "        img_name = os.path.join(self.img_dir, self.tree_frame.iloc[idx, 0])\n",
    "        image = Image.open(img_name)\n",
    "        label = int(self.tree_frame.iloc[idx, 1])\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "      \n",
    "        return image, label\n",
    "\n",
    "# Transformation for the image\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((600, 600)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# Loaders\n",
    "loaders = {}\n",
    "\n",
    "for phase in ['train', 'valid', 'test']:\n",
    "    dir_path = os.path.join(base_dir, sub_dirs[phase])\n",
    "    csv_file = os.path.join(dir_path, \"_annotations.coco.csv\") #replace annotation file name for your annotation file name\n",
    "    \n",
    "    dataset = TreeDataset(csv_file=csv_file, img_dir=os.path.join(dir_path, 'images'), transform=transform)\n",
    "    \n",
    "    if phase == 'train':\n",
    "        batch_size = 16  # For training\n",
    "    else:\n",
    "        batch_size = 16  # For validation/testing to reduce memory usage\n",
    "    \n",
    "    loaders[phase] = DataLoader(dataset, batch_size=batch_size, shuffle=True if phase == 'train' else False)\n",
    "\n",
    "# Now, you can use loaders['train'], loaders['valid'], and loaders['test'] for your training, validation, and testing loops.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90b91030",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-02T09:30:41.720038Z",
     "iopub.status.busy": "2024-04-02T09:30:41.719649Z",
     "iopub.status.idle": "2024-04-02T09:48:44.393523Z",
     "shell.execute_reply": "2024-04-02T09:48:44.392601Z",
     "shell.execute_reply.started": "2024-04-02T09:30:41.720008Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from torch.nn.modules.loss import BCEWithLogitsLoss\n",
    "from torch.optim import lr_scheduler\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import models, transforms\n",
    "import torch.nn as nn\n",
    "from torch.nn.modules.loss import BCEWithLogitsLoss\n",
    "from torch.optim import Adam\n",
    "from tqdm import tqdm\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "writer = SummaryWriter()\n",
    "\n",
    "# Initialize model\n",
    "model = models.resnet18(pretrained=True)\n",
    "nr_filters = model.fc.in_features  # number of input features of the last layer\n",
    "model.fc = nn.Linear(nr_filters, 1)  # Adjusting for binary classification\n",
    "model = model.to(device)\n",
    "\n",
    "# Loss and optimizer\n",
    "loss_fn = BCEWithLogitsLoss()\n",
    "optimizer = Adam(model.fc.parameters(), lr=0.01)\n",
    "\n",
    "# Training step function\n",
    "def make_train_step(model, optimizer, loss_fn):\n",
    "    def train_step(x, y):\n",
    "        model.train()  # Enter train mode\n",
    "        yhat = model(x)  # Make prediction\n",
    "        loss = loss_fn(yhat, y)  # Compute loss\n",
    "        loss.backward()  # Backpropagate the gradients\n",
    "        optimizer.step()  # Update parameters\n",
    "        optimizer.zero_grad()  # Reset gradients\n",
    "        \n",
    "        # Calculate accuracy\n",
    "        yhat_sig = torch.sigmoid(yhat)\n",
    "        acc = accuracy(yhat_sig, y)\n",
    "        return loss.item(), acc\n",
    "    return train_step\n",
    "\n",
    "# Accuracy calculation\n",
    "def accuracy(preds, labels):\n",
    "    preds_rounded = torch.round(torch.sigmoid(preds))\n",
    "    correct = (preds_rounded == labels).float()  # convert into float for division\n",
    "    acc = correct.sum() / len(correct)\n",
    "    \n",
    "    return acc.item()\n",
    "\n",
    "\n",
    "# Prepare dataset and dataloaders\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((600, 600)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "### directories should be changed to directories as decribed in first cell of this notebook\n",
    "train_csv = '/kaggle/input/tree-or-no-tree-labled/TreeOrNoTree-2/train/_annotations.coco.csv'\n",
    "train_dir = '/kaggle/input/tree-or-no-tree-labled/TreeOrNoTree-2/train'\n",
    "valid_csv = '/kaggle/input/tree-or-no-tree-labled/TreeOrNoTree-2/valid/_annotations.coco.csv'\n",
    "valid_dir = '/kaggle/input/tree-or-no-tree-labled/TreeOrNoTree-2/valid'\n",
    "\n",
    "train_dataset = TreeDataset(csv_file=train_csv, img_dir=train_dir, transform=transform)\n",
    "valid_dataset = TreeDataset(csv_file=valid_csv, img_dir=valid_dir, transform=transform)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "valid_loader = DataLoader(valid_dataset, batch_size=16)\n",
    "\n",
    "# Train step\n",
    "train_step = make_train_step(model, optimizer, loss_fn)\n",
    "\n",
    "# Training loop\n",
    "n_epochs = 50\n",
    "best_valid_acc = 0.0  # Initialize with a baseline accuracy\n",
    "for epoch in range(n_epochs):\n",
    "    # Training\n",
    "    model.train()\n",
    "    train_losses, train_accs = [], []\n",
    "    for images, labels in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{n_epochs}, Train\"):\n",
    "        images, labels = images.to(device), labels.to(device).unsqueeze(1).float()\n",
    "        loss, acc = train_step(images, labels)\n",
    "        train_losses.append(loss)\n",
    "        train_accs.append(acc)\n",
    "    \n",
    "    avg_train_loss = sum(train_losses) / len(train_losses)\n",
    "    avg_train_acc = sum(train_accs) / len(train_accs)\n",
    "    print(f\"Training loss: {avg_train_loss:.4f}, Accuracy: {avg_train_acc:.4f}\")\n",
    "    \n",
    "    # Validation\n",
    "    model.eval()\n",
    "    valid_losses, valid_accs = [], []\n",
    "    with torch.no_grad():\n",
    "        for images, labels in tqdm(valid_loader, desc=f\"Epoch {epoch+1}/{n_epochs}, Valid\"):\n",
    "            images, labels = images.to(device), labels.to(device).unsqueeze(1).float()\n",
    "            outputs = model(images)\n",
    "            loss = loss_fn(outputs, labels)\n",
    "            acc = accuracy(outputs, labels)\n",
    "            valid_losses.append(loss.item())\n",
    "            valid_accs.append(acc)\n",
    "    \n",
    "    avg_valid_loss = sum(valid_losses) / len(valid_losses)\n",
    "    avg_valid_acc = sum(valid_accs) / len(valid_accs)\n",
    "    print(f\"Validation loss: {avg_valid_loss:.4f}, Accuracy: {avg_valid_acc:.4f}\")\n",
    "    \n",
    "    if avg_valid_acc > best_valid_acc:\n",
    "        best_valid_acc = avg_valid_acc\n",
    "        model_save_path = os.path.join(writer.log_dir, 'best_model.pth')\n",
    "        torch.save(model.state_dict(), model_save_path)\n",
    "        print(f\"Saved Best Model at {model_save_path}\")\n",
    "        \n",
    "    writer.add_scalar('Loss/train', avg_train_loss, epoch)\n",
    "    writer.add_scalar('Accuracy/train', avg_train_acc, epoch)\n",
    "    writer.add_scalar('Loss/valid', avg_valid_loss, epoch)\n",
    "    writer.add_scalar('Accuracy/valid', avg_valid_acc, epoch)\n",
    "\n",
    "writer.close()  # Close the TensorBoard SummaryWriter  \n",
    "torch.save(model.state_dict(), f'model_epoch_{epoch+1}.pth')\n",
    "\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa87d9f8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-02T08:58:42.343468Z",
     "iopub.status.busy": "2024-04-02T08:58:42.342799Z",
     "iopub.status.idle": "2024-04-02T08:58:42.350000Z",
     "shell.execute_reply": "2024-04-02T08:58:42.349126Z",
     "shell.execute_reply.started": "2024-04-02T08:58:42.343433Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "len(train_dataset)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ba383d1",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "# Testing The model on test dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b7a1a18",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-02T09:51:36.938396Z",
     "iopub.status.busy": "2024-04-02T09:51:36.937693Z",
     "iopub.status.idle": "2024-04-02T09:51:39.016467Z",
     "shell.execute_reply": "2024-04-02T09:51:39.015538Z",
     "shell.execute_reply.started": "2024-04-02T09:51:36.938362Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision import models, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.nn.modules.loss import BCEWithLogitsLoss\n",
    "\n",
    "# Assuming TreeDataset is defined elsewhere and properly imported here\n",
    "\n",
    "# Define device\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# Initialize model (ensure this matches the model used for training)\n",
    "model = models.resnet18(pretrained=True)\n",
    "nr_filters = model.fc.in_features\n",
    "model.fc = torch.nn.Linear(nr_filters, 1)\n",
    "model = model.to(device)\n",
    "\n",
    "# Load trained model weights (replace 'path_to_trained_model_weights.pth' with your actual model's saved weights)\n",
    "model.load_state_dict(torch.load('/kaggle/working/model_epoch_50.pth', map_location=device))\n",
    "model.eval()\n",
    "\n",
    "# Define the test dataset and dataloader\n",
    "test_transform = transforms.Compose([\n",
    "    transforms.Resize((600, 600)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "test_csv = '/kaggle/input/tree-or-no-tree-labled/TreeOrNoTree-2/test/_annotations.coco.csv'\n",
    "test_dir = '/kaggle/input/tree-or-no-tree-labled/TreeOrNoTree-2/test'\n",
    "\n",
    "test_dataset = TreeDataset(csv_file=test_csv, img_dir=test_dir, transform=test_transform)\n",
    "test_loader = DataLoader(test_dataset, batch_size=16, shuffle=False)\n",
    "\n",
    "# Function to calculate accuracy\n",
    "def accuracy(preds, labels):\n",
    "    preds_rounded = torch.round(torch.sigmoid(preds))\n",
    "    correct = (preds_rounded == labels).float()  # convert into float for division \n",
    "    acc = correct.sum() / len(correct)\n",
    "    return acc.item()\n",
    "\n",
    "# Testing loop\n",
    "with torch.no_grad():\n",
    "    test_losses = []\n",
    "    test_accs = []\n",
    "    loss_fn = BCEWithLogitsLoss()\n",
    "\n",
    "    for images, labels in test_loader:\n",
    "        images, labels = images.to(device), labels.to(device).unsqueeze(1).float()\n",
    "        outputs = model(images)\n",
    "        loss = loss_fn(outputs, labels)\n",
    "\n",
    "        test_losses.append(loss.item())\n",
    "        test_accs.append(accuracy(outputs, labels))\n",
    "\n",
    "    avg_test_loss = sum(test_losses) / len(test_losses)\n",
    "    avg_test_acc = sum(test_accs) / len(test_accs)\n",
    "    print(f\"Test loss: {avg_test_loss:.4f}, Accuracy: {avg_test_acc:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82f6344b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-02T10:02:11.455239Z",
     "iopub.status.busy": "2024-04-02T10:02:11.454775Z",
     "iopub.status.idle": "2024-04-02T10:02:13.548997Z",
     "shell.execute_reply": "2024-04-02T10:02:13.548120Z",
     "shell.execute_reply.started": "2024-04-02T10:02:11.455203Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "def show_images_with_predictions(images, outputs, labels):\n",
    "    fig, axs = plt.subplots(1, 5, figsize=(20, 4))\n",
    "    for i, ax in enumerate(axs.flatten()):\n",
    "        img = np.transpose(images[i], (1, 2, 0))\n",
    "        img = img * np.array([0.229, 0.224, 0.225]) + np.array([0.485, 0.456, 0.406])  # Unnormalize\n",
    "        img = np.clip(img, 0, 1)\n",
    "        \n",
    "        ax.imshow(img)\n",
    "        predicted_label = '1' if outputs[i] > 0.5 else '0'\n",
    "        true_label = '1' if labels[i] > 0.5 else '0'\n",
    "        ax.text(0, -5, f'Pred: {predicted_label} - True: {true_label}', color='red', fontsize=12, backgroundcolor='white')\n",
    "        ax.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "# Randomly select a batch from the test loader\n",
    "random_batch = random.choice([x for x in test_loader])\n",
    "images, labels = random_batch\n",
    "images, labels = images.to(device), labels.to(device).unsqueeze(1).float()\n",
    "outputs = model(images)\n",
    "outputs_sigmoid = torch.sigmoid(outputs).cpu().detach().numpy()\n",
    "labels = labels.cpu().detach().numpy()\n",
    "\n",
    "# Select 10 random images from the batch\n",
    "idxs = np.random.choice(images.size(0), 5)\n",
    "selected_images = images[idxs].cpu().numpy()\n",
    "selected_outputs = outputs_sigmoid[idxs]\n",
    "selected_labels = labels[idxs]\n",
    "\n",
    "show_images_with_predictions(selected_images, selected_outputs, selected_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b7d6bfc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-02T10:15:18.440593Z",
     "iopub.status.busy": "2024-04-02T10:15:18.439970Z",
     "iopub.status.idle": "2024-04-02T10:15:39.548957Z",
     "shell.execute_reply": "2024-04-02T10:15:39.547666Z",
     "shell.execute_reply.started": "2024-04-02T10:15:18.440559Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load the TensorBoard notebook extension (might not be necessary or effective in Kaggle)\n",
    "%load_ext tensorboard\n",
    "\n",
    "# Launch TensorBoard pointing to the directory where logs are saved\n",
    "%tensorboard --logdir '/kaggle/working/runs/Apr02_09-30-51_0c98c022bac7'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c14bc2c4",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 4714367,
     "sourceId": 8004894,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30673,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 3.709834,
   "end_time": "2024-04-02T10:16:01.265262",
   "environment_variables": {},
   "exception": true,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2024-04-02T10:15:57.555428",
   "version": "2.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
