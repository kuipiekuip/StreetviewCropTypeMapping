{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Creating Lables\n",
    "\n",
    "this code creates lables in the TreeOrNoTree-2 folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed /Users/marco/Library/CloudStorage/OneDrive-DelftUniversityofTechnology/Control&Simulation/deeplearning/github/StreetviewCropTypeMapping/TreeOrNoTree-2/test/_annotations.coco.json -> /Users/marco/Library/CloudStorage/OneDrive-DelftUniversityofTechnology/Control&Simulation/deeplearning/github/StreetviewCropTypeMapping/TreeOrNoTree-2/test/_annotations.coco.csv\n",
      "Processed /Users/marco/Library/CloudStorage/OneDrive-DelftUniversityofTechnology/Control&Simulation/deeplearning/github/StreetviewCropTypeMapping/TreeOrNoTree-2/train/_annotations.coco.json -> /Users/marco/Library/CloudStorage/OneDrive-DelftUniversityofTechnology/Control&Simulation/deeplearning/github/StreetviewCropTypeMapping/TreeOrNoTree-2/train/_annotations.coco.csv\n",
      "Processed /Users/marco/Library/CloudStorage/OneDrive-DelftUniversityofTechnology/Control&Simulation/deeplearning/github/StreetviewCropTypeMapping/TreeOrNoTree-2/valid/_annotations.coco.json -> /Users/marco/Library/CloudStorage/OneDrive-DelftUniversityofTechnology/Control&Simulation/deeplearning/github/StreetviewCropTypeMapping/TreeOrNoTree-2/valid/_annotations.coco.csv\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import csv\n",
    "\n",
    "# Function to extract filenames and labels from JSON annotations\n",
    "def extract_filenames_and_labels(json_file):\n",
    "    with open(json_file) as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    filenames = []\n",
    "    labels = []\n",
    "\n",
    "    # Map of image IDs to filenames\n",
    "    image_id_to_filename = {image['id']: image['file_name'] for image in data['images']}\n",
    "\n",
    "    # Set of image IDs that have trees\n",
    "    image_ids_with_trees = set(annotation['image_id'] for annotation in data['annotations'] if annotation['category_id'] == 1)\n",
    "\n",
    "    # Generate filenames and labels\n",
    "    for image_id, filename in image_id_to_filename.items():\n",
    "        filenames.append(filename)\n",
    "        labels.append(1 if image_id in image_ids_with_trees else 0)\n",
    "\n",
    "    return filenames, labels\n",
    "\n",
    "# Main function to process directories and generate CSV files\n",
    "def process_directories(base_dir):\n",
    "    # Sub-directories to process\n",
    "    sub_dirs = ['test', 'train', 'valid']\n",
    "\n",
    "    for sub_dir in sub_dirs:\n",
    "        current_dir = os.path.join(base_dir, sub_dir)\n",
    "        \n",
    "        # Process each JSON file in the directory\n",
    "        for file in os.listdir(current_dir):\n",
    "            if file.endswith('.json'):\n",
    "                json_file_path = os.path.join(current_dir, file)\n",
    "                filenames, labels = extract_filenames_and_labels(json_file_path)\n",
    "\n",
    "                # Create a corresponding CSV file\n",
    "                csv_file_path = os.path.join(current_dir, os.path.splitext(file)[0] + '.csv')\n",
    "                with open(csv_file_path, mode='w', newline='') as csv_file:\n",
    "                    writer = csv.writer(csv_file)\n",
    "                    writer.writerow(['filename', 'label'])  # Write header\n",
    "                    for filename, label in zip(filenames, labels):\n",
    "                        writer.writerow([filename, label])\n",
    "\n",
    "                print(f\"Processed {json_file_path} -> {csv_file_path}\")\n",
    "\n",
    "# Example usage\n",
    "base_dir = '/Users/marco/Library/CloudStorage/OneDrive-DelftUniversityofTechnology/Control&Simulation/deeplearning/github/StreetviewCropTypeMapping/TreeOrNoTree-2'  # Update this to your base directory path\n",
    "process_directories(base_dir)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import csv\n",
    "import torch\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from PIL import Image\n",
    "\n",
    "# Define paths\n",
    "base_dir = \"/Users/marco/Library/CloudStorage/OneDrive-DelftUniversityofTechnology/Control&Simulation/deeplearning/github/StreetviewCropTypeMapping/TreeOrNoTree-2\"\n",
    "sub_dirs = {\n",
    "    \"train\": \"train\",\n",
    "    \"valid\": \"valid\",\n",
    "    \"test\": \"test\"\n",
    "}\n",
    "\n",
    "# Custom Dataset Class\n",
    "class TreeDataset(Dataset):\n",
    "    def __init__(self, csv_file, img_dir, transform=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            csv_file (string): Path to the csv file with annotations.\n",
    "            img_dir (string): Directory with all the images.\n",
    "            transform (callable, optional): Optional transform to be applied\n",
    "                on a sample.\n",
    "        \"\"\"\n",
    "        self.tree_frame = pd.read_csv(csv_file)\n",
    "        self.img_dir = img_dir\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.tree_frame)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "\n",
    "        img_name = os.path.join(self.img_dir, self.tree_frame.iloc[idx, 0])\n",
    "        image = Image.open(img_name)\n",
    "        label = int(self.tree_frame.iloc[idx, 1])\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return image, label\n",
    "\n",
    "# Transformation for the image\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((600, 600)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# Loaders\n",
    "loaders = {}\n",
    "\n",
    "for phase in ['train', 'valid', 'test']:\n",
    "    dir_path = os.path.join(base_dir, sub_dirs[phase])\n",
    "    csv_file = os.path.join(dir_path, \"_annotations.coco.csv\")\n",
    "    \n",
    "    dataset = TreeDataset(csv_file=csv_file, img_dir=os.path.join(dir_path, 'images'), transform=transform)\n",
    "    \n",
    "    if phase == 'train':\n",
    "        batch_size = 32  # For training\n",
    "    else:\n",
    "        batch_size = 16  # For validation/testing to reduce memory usage\n",
    "    \n",
    "    loaders[phase] = DataLoader(dataset, batch_size=batch_size, shuffle=True if phase == 'train' else False)\n",
    "\n",
    "# Now, you can use loaders['train'], loaders['valid'], and loaders['test'] for your training, validation, and testing loops.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/20, Train:   0%|          | 0/22 [00:43<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[39], line 77\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m images, labels \u001b[38;5;129;01min\u001b[39;00m tqdm(train_loader, desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mn_epochs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Train\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m     76\u001b[0m     images, labels \u001b[38;5;241m=\u001b[39m images\u001b[38;5;241m.\u001b[39mto(device), labels\u001b[38;5;241m.\u001b[39mto(device)\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mfloat()\n\u001b[0;32m---> 77\u001b[0m     loss, acc \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     78\u001b[0m     train_losses\u001b[38;5;241m.\u001b[39mappend(loss)\n\u001b[1;32m     79\u001b[0m     train_accs\u001b[38;5;241m.\u001b[39mappend(acc)\n",
      "Cell \u001b[0;32mIn[39], line 27\u001b[0m, in \u001b[0;36mmake_train_step.<locals>.train_step\u001b[0;34m(x, y)\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtrain_step\u001b[39m(x, y):\n\u001b[1;32m     26\u001b[0m     model\u001b[38;5;241m.\u001b[39mtrain()  \u001b[38;5;66;03m# Enter train mode\u001b[39;00m\n\u001b[0;32m---> 27\u001b[0m     yhat \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Make prediction\u001b[39;00m\n\u001b[1;32m     28\u001b[0m     loss \u001b[38;5;241m=\u001b[39m loss_fn(yhat, y)  \u001b[38;5;66;03m# Compute loss\u001b[39;00m\n\u001b[1;32m     29\u001b[0m     loss\u001b[38;5;241m.\u001b[39mbackward()  \u001b[38;5;66;03m# Backpropagate the gradients\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/deeplearning_cropfinder/lib/python3.11/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/deeplearning_cropfinder/lib/python3.11/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/deeplearning_cropfinder/lib/python3.11/site-packages/torchvision/models/resnet.py:285\u001b[0m, in \u001b[0;36mResNet.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    284\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 285\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_forward_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/deeplearning_cropfinder/lib/python3.11/site-packages/torchvision/models/resnet.py:274\u001b[0m, in \u001b[0;36mResNet._forward_impl\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    271\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmaxpool(x)\n\u001b[1;32m    273\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayer1(x)\n\u001b[0;32m--> 274\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlayer2\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    275\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayer3(x)\n\u001b[1;32m    276\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayer4(x)\n",
      "File \u001b[0;32m~/miniconda3/envs/deeplearning_cropfinder/lib/python3.11/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/deeplearning_cropfinder/lib/python3.11/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/deeplearning_cropfinder/lib/python3.11/site-packages/torch/nn/modules/container.py:217\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    215\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    216\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 217\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    218\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m~/miniconda3/envs/deeplearning_cropfinder/lib/python3.11/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/deeplearning_cropfinder/lib/python3.11/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/deeplearning_cropfinder/lib/python3.11/site-packages/torchvision/models/resnet.py:150\u001b[0m, in \u001b[0;36mBottleneck.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    147\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbn1(out)\n\u001b[1;32m    148\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrelu(out)\n\u001b[0;32m--> 150\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv2\u001b[49m\u001b[43m(\u001b[49m\u001b[43mout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    151\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbn2(out)\n\u001b[1;32m    152\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrelu(out)\n",
      "File \u001b[0;32m~/miniconda3/envs/deeplearning_cropfinder/lib/python3.11/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/deeplearning_cropfinder/lib/python3.11/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/deeplearning_cropfinder/lib/python3.11/site-packages/torch/nn/modules/conv.py:460\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    459\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 460\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_conv_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/deeplearning_cropfinder/lib/python3.11/site-packages/torch/nn/modules/conv.py:456\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    452\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mzeros\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m    453\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mconv2d(F\u001b[38;5;241m.\u001b[39mpad(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode),\n\u001b[1;32m    454\u001b[0m                     weight, bias, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstride,\n\u001b[1;32m    455\u001b[0m                     _pair(\u001b[38;5;241m0\u001b[39m), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdilation, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroups)\n\u001b[0;32m--> 456\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv2d\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    457\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroups\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from torch.nn.modules.loss import BCEWithLogitsLoss\n",
    "from torch.optim import lr_scheduler\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import models, transforms\n",
    "import torch.nn as nn\n",
    "from torch.nn.modules.loss import BCEWithLogitsLoss\n",
    "from torch.optim import Adam\n",
    "from tqdm import tqdm\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# Initialize model\n",
    "model = models.resnet50(pretrained=True)\n",
    "nr_filters = model.fc.in_features  # number of input features of the last layer\n",
    "model.fc = nn.Linear(nr_filters, 1)  # Adjusting for binary classification\n",
    "model = model.to(device)\n",
    "\n",
    "# Loss and optimizer\n",
    "loss_fn = BCEWithLogitsLoss()\n",
    "optimizer = Adam(model.fc.parameters(), lr=0.01)\n",
    "\n",
    "# Training step function\n",
    "def make_train_step(model, optimizer, loss_fn):\n",
    "    def train_step(x, y):\n",
    "        model.train()  # Enter train mode\n",
    "        yhat = model(x)  # Make prediction\n",
    "        loss = loss_fn(yhat, y)  # Compute loss\n",
    "        loss.backward()  # Backpropagate the gradients\n",
    "        optimizer.step()  # Update parameters\n",
    "        optimizer.zero_grad()  # Reset gradients\n",
    "        \n",
    "        # Calculate accuracy\n",
    "        yhat_sig = torch.sigmoid(yhat)\n",
    "        acc = accuracy(yhat_sig, y)\n",
    "        return loss.item(), acc\n",
    "    return train_step\n",
    "\n",
    "# Accuracy calculation\n",
    "def accuracy(preds, labels):\n",
    "    preds_rounded = torch.round(torch.sigmoid(preds))\n",
    "    correct = (preds_rounded == labels).float()  # convert into float for division\n",
    "    acc = correct.sum() / len(correct)\n",
    "    return acc.item()\n",
    "\n",
    "\n",
    "# Prepare dataset and dataloaders\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((600, 600)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "\n",
    "train_csv = '/Users/marco/Library/CloudStorage/OneDrive-DelftUniversityofTechnology/Control&Simulation/deeplearning/github/StreetviewCropTypeMapping/TreeOrNoTree-2/train/_annotations.coco.csv'\n",
    "train_dir = '/Users/marco/Library/CloudStorage/OneDrive-DelftUniversityofTechnology/Control&Simulation/deeplearning/github/StreetviewCropTypeMapping/TreeOrNoTree-2/train'\n",
    "valid_csv = '/Users/marco/Library/CloudStorage/OneDrive-DelftUniversityofTechnology/Control&Simulation/deeplearning/github/StreetviewCropTypeMapping/TreeOrNoTree-2/valid/_annotations.coco.csv'\n",
    "valid_dir = '/Users/marco/Library/CloudStorage/OneDrive-DelftUniversityofTechnology/Control&Simulation/deeplearning/github/StreetviewCropTypeMapping/TreeOrNoTree-2/valid'\n",
    "\n",
    "train_dataset = TreeDataset(csv_file=train_csv, img_dir=train_dir, transform=transform)\n",
    "valid_dataset = TreeDataset(csv_file=valid_csv, img_dir=valid_dir, transform=transform)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "valid_loader = DataLoader(valid_dataset, batch_size=16)\n",
    "\n",
    "# Train step\n",
    "train_step = make_train_step(model, optimizer, loss_fn)\n",
    "\n",
    "# Training loop\n",
    "n_epochs = 20\n",
    "for epoch in range(n_epochs):\n",
    "    # Training\n",
    "    model.train()\n",
    "    train_losses, train_accs = [], []\n",
    "    for images, labels in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{n_epochs}, Train\"):\n",
    "        images, labels = images.to(device), labels.to(device).unsqueeze(1).float()\n",
    "        loss, acc = train_step(images, labels)\n",
    "        train_losses.append(loss)\n",
    "        train_accs.append(acc)\n",
    "    \n",
    "    avg_train_loss = sum(train_losses) / len(train_losses)\n",
    "    avg_train_acc = sum(train_accs) / len(train_accs)\n",
    "    print(f\"Training loss: {avg_train_loss:.4f}, Accuracy: {avg_train_acc:.4f}\")\n",
    "    \n",
    "    # Validation\n",
    "    model.eval()\n",
    "    valid_losses, valid_accs = [], []\n",
    "    with torch.no_grad():\n",
    "        for images, labels in tqdm(valid_loader, desc=f\"Epoch {epoch+1}/{n_epochs}, Valid\"):\n",
    "            images, labels = images.to(device), labels.to(device).unsqueeze(1).float()\n",
    "            outputs = model(images)\n",
    "            loss = loss_fn(outputs, labels)\n",
    "            acc = accuracy(outputs, labels)\n",
    "            valid_losses.append(loss.item())\n",
    "            valid_accs.append(acc)\n",
    "    \n",
    "    avg_valid_loss = sum(valid_losses) / len(valid_losses)\n",
    "    avg_valid_acc = sum(valid_accs) / len(valid_accs)\n",
    "    print(f\"Validation loss: {avg_valid_loss:.4f}, Accuracy: {avg_valid_acc:.4f}\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_score, recall_score\n",
    "\n",
    "# print('Labels', np.array(labels).shape)\n",
    "\n",
    "\n",
    "print(np.array(testloader))\n",
    "\n",
    "def make_train_step(model, optimizer, loss_fn):\n",
    "  def train_step(x,y):\n",
    "    #make prediction\n",
    "    yhat = model(x)\n",
    "    #enter train mode\n",
    "    model.train()\n",
    "    #compute loss\n",
    "    loss = loss_fn(yhat,y)\n",
    "\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad()\n",
    "    #optimizer.cleargrads()\n",
    "    yhatsig = torch.sigmoid(yhat)\n",
    "    acc = accuracy(yhatsig, y)\n",
    "\n",
    "    return loss, acc\n",
    "  return train_step\n",
    "\n",
    "def accuracy(preds, labels):\n",
    "    acc = 0\n",
    "    for i, pred in enumerate(preds):\n",
    "        p = torch.argmax(pred)\n",
    "\n",
    "        if torch.round(pred) == labels[i]:\n",
    "            acc +=1\n",
    "\n",
    "    return acc/len(preds)\n",
    "\n",
    "from torch.nn.modules.loss import BCEWithLogitsLoss\n",
    "from torch.optim import lr_scheduler\n",
    "\n",
    "#loss\n",
    "loss_fn = BCEWithLogitsLoss() #binary cross entropy with sigmoid, so no need to use sigmoid in the model\n",
    "\n",
    "#optimizer\n",
    "optimizer = torch.optim.Adam(model.fc.parameters(), lr=0.01, betas=(0.9, 0.999))\n",
    "\n",
    "#train step\n",
    "train_step = make_train_step(model, optimizer, loss_fn)\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "losses = []\n",
    "val_losses = []\n",
    "accs = []\n",
    "val_accs = []\n",
    "epoch_train_losses = []\n",
    "epoch_test_losses = []\n",
    "epoch_train_accs = []\n",
    "epoch_test_accs = []\n",
    "\n",
    "n_epochs = 1\n",
    "early_stopping_tolerance = 4\n",
    "early_stopping_threshold = 1.0\n",
    "y_true = []\n",
    "y_pred = []\n",
    "for epoch in range(n_epochs):\n",
    "  epoch_loss = 0\n",
    "  epoch_acc = 0\n",
    "  print(epoch)\n",
    "  print(len(trainloader))\n",
    "\n",
    "  #validation doesnt requires gradient\n",
    "  with torch.no_grad():\n",
    "    cum_loss = 0\n",
    "    cum_acc = 0\n",
    "\n",
    "    for x_batch, y_batch in testloader:\n",
    "      x_batch = x_batch.to(device)\n",
    "      y_batch = y_batch.unsqueeze(1).float() #convert target to same nn output shape\n",
    "      y_batch = y_batch.to(device)\n",
    "\n",
    "      #model to eval mode\n",
    "      model.eval()\n",
    "\n",
    "      yhat = model(x_batch)\n",
    "      yhatsig = torch.sigmoid(yhat)\n",
    "      # for i in range(len(yhatsig)):\n",
    "      #   print(yhatsig[i], y_batch[i])\n",
    "      y_true.extend(y_batch.cpu().numpy())  # Append true labels\n",
    "      y_pred.extend(yhatsig.cpu().numpy())  # Append predicted labels\n",
    "      val_loss = loss_fn(yhat,y_batch)\n",
    "      cum_loss += val_loss/len(testloader)\n",
    "      val_losses.append(val_loss.item())\n",
    "\n",
    "      val_acc = accuracy(yhatsig, y_batch)\n",
    "      cum_acc += val_acc/len(testloader)\n",
    "      val_accs.append(val_acc)\n",
    "\n",
    "\n",
    "    epoch_test_losses.append(cum_loss)\n",
    "    epoch_test_accs.append(cum_acc)\n",
    "\n",
    "    print('Epoch : {}, test loss : {} test acc: {}'.format(epoch+1,cum_loss, cum_acc))\n",
    "    print(np.count_nonzero(np.array(y_pred) > 0.5))\n",
    "    print(y_true.count(1))\n",
    "    # print((np.array(y_pred)).astype(int))\n",
    "    precision = precision_score(y_true, (np.array(y_pred)).astype(int))\n",
    "    recall = recall_score(y_true, (np.array(y_pred)).astype(int))\n",
    "    print(\"Precision\", precision)\n",
    "    print(\"Recall\", recall)\n",
    "\n",
    "    best_loss = min(epoch_test_losses)\n",
    "    best_acc = max(epoch_test_accs)\n",
    "    #save best model\n",
    "    if cum_acc >= best_acc:\n",
    "      best_acc_model_wts = model.state_dict()\n",
    "    if cum_loss <= best_loss:\n",
    "      best_loss_model_wts = model.state_dict()\n",
    "\n",
    "    #early stopping\n",
    "    early_stopping_counter = 0\n",
    "    if cum_acc < best_acc:\n",
    "      early_stopping_counter +=1\n",
    "\n",
    "    if (early_stopping_counter == early_stopping_tolerance) or (best_acc >= early_stopping_threshold):\n",
    "      print(\"/nTerminating: early stopping\")\n",
    "      break #terminate training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def testModels(model, testloader):\n",
    "  with torch.no_grad():\n",
    "    cum_loss = 0\n",
    "    cum_acc = 0\n",
    "    for x_batch, y_batch in testloader:\n",
    "      x_batch = x_batch.to(device)\n",
    "      y_batch = y_batch.unsqueeze(1).float() #convert target to same nn output shape\n",
    "      y_batch = y_batch.to(device)\n",
    "\n",
    "      #model to eval mode\n",
    "      model.eval()\n",
    "\n",
    "      yhat = model(x_batch)\n",
    "      yhatsig = torch.sigmoid(yhat)\n",
    "\n",
    "      test_loss = loss_fn(yhat,y_batch)\n",
    "      cum_loss += test_loss/len(testloader)\n",
    "\n",
    "      test_acc = accuracy(yhatsig, y_batch)\n",
    "      cum_acc += test_acc/len(testloader)\n",
    "\n",
    "    print('Test loss : {} test acc: {}'.format(cum_loss, cum_acc))\n",
    "\n",
    "# model.load_state_dict(best_acc_model_wts)\n",
    "testModels(model, testloader)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference Section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def saveModel(model, PATH):\n",
    "  torch.save(model.state_dict(), PATH)\n",
    "\n",
    "\n",
    "def loadModel(PATH):\n",
    "  # model = resnet18(pretrained=True, num_classes=4)  # where num_classes will be different\n",
    "\n",
    "  model = models.resnet18(pretrained=True)\n",
    "  nr_filters = model.fc.in_features\n",
    "  model.fc = nn.Linear(nr_filters, 1)\n",
    "  model.load_state_dict(torch.load(PATH))\n",
    "  model.eval()\n",
    "  return model\n",
    "\n",
    "PATH = imagesRoot + \"fieldOrNot-ResNet18-87%.pt\"\n",
    "model = loadModel(PATH)\n",
    "\n",
    "# saveModel(model, PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "folderPath = imagesRoot+\"Thailand16/\"\n",
    "filteredFilenames = os.listdir(folderPath)\n",
    "print(len(filteredFilenames))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outFolderPath = imagesRoot +'ThailandFieldOrNot/'\n",
    "classes = {0: 'field/', 1: 'notField/'}\n",
    "out = os.listdir(outFolderPath+'field/')\n",
    "print(len(out))\n",
    "out += os.listdir(outFolderPath+'notField/')\n",
    "\n",
    "allfiles = [x for x in filteredFilenames if x not in out]\n",
    "print(len(allfiles))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import concurrent.futures\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, folder_path, filenames):\n",
    "        self.folder_path = folder_path\n",
    "        self.filenames = [fn for fn in filenames if fn.lower().endswith('.jpg')]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.filenames)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        file_path = os.path.join(self.folder_path, self.filenames[idx])\n",
    "        image = torchvision.io.read_image(file_path).to(torch.float)\n",
    "        return image, self.filenames[idx]\n",
    "\n",
    "def save_image(out_folder, class_folder, filename, image):\n",
    "    out_path = os.path.join(out_folder, class_folder, filename)\n",
    "    image = Image.fromarray(image)\n",
    "    image.save(out_path)\n",
    "\n",
    "def saveModelPreds(folderPath, outFolderPath, filenames, classes, numSaved=5):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    dataset = CustomDataset(folderPath, filenames)\n",
    "    imsLoader = DataLoader(dataset, batch_size=32)\n",
    "\n",
    "    model.eval()\n",
    "    outs = 0\n",
    "    model.to(device)\n",
    "    with torch.no_grad(), concurrent.futures.ThreadPoolExecutor() as executor:\n",
    "        for x_batch, fils in tqdm(imsLoader, total=len(imsLoader)):\n",
    "            x_batch = x_batch.to(device)\n",
    "            # print(x_batch.shape)\n",
    "            yhat = model(x_batch)\n",
    "            yhatsig = torch.sigmoid(yhat).cpu().numpy()\n",
    "            # print(np.rint(yhatsig[:, 0]).astype(int))\n",
    "            classFolders = [classes[int(index)] for index in np.rint(yhatsig[:, 0])]\n",
    "            im_batch = x_batch.permute(0, 2, 3, 1).cpu().numpy().astype(np.uint8)\n",
    "\n",
    "            for j in range(len(x_batch)):\n",
    "                future = save_image(outFolderPath, classFolders[j], fils[j], im_batch[j])\n",
    "                outs += 1\n",
    "\n",
    "    print(\"Images Classified:\", outs)\n",
    "\n",
    "outFolderPath = imagesRoot +'ThailandFieldOrNot/'\n",
    "folderPath = imagesRoot+\"Thailand15/\"\n",
    "\n",
    "classes = {0: 'field/', 1: 'notField/'}\n",
    "# saveModelPreds(folderPath, outFolderPath, filteredFilenames, classes)\n",
    "saveModelPreds(folderPath, outFolderPath, allfiles, classes)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deeplearning_cropfinder",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.1.undefined"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
